{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Пожалуйста, выполните код ниже, дабы скачать в коллаб нужные файлы.\n",
    "#Если вы не в коллабе, выполнять код не нужно\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vika/embedding_vs_bert\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('/home/vika/embedding_vs_bert/data/review_books.tsv', sep='\\t') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = df[df['score'] == 1]\n",
    "neg = df[df['score'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78994 21008 100002\n"
     ]
    }
   ],
   "source": [
    "print (len(pos), len(neg), len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = pos[:21000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train = pos[:20000].dropna()\n",
    "pos_test = pos[21000:].dropna()\n",
    "neg_train = neg[:20000].dropna()\n",
    "neg_test = neg[21000:].dropna()\n",
    "\n",
    "train = pd.concat([pos_train, neg_train])\n",
    "train = train.reindex(np.random.permutation(train.index))\n",
    "test = pd.concat([pos_test, neg_test])\n",
    "test = test.reindex(np.random.permutation(test.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15978</th>\n",
       "      <td>0</td>\n",
       "      <td>Take a person who prides herself on having sex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036</th>\n",
       "      <td>0</td>\n",
       "      <td>I always had a vague idea as to what the Klond...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66996</th>\n",
       "      <td>0</td>\n",
       "      <td>I just couldn't do it. We bought this book bas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17660</th>\n",
       "      <td>0</td>\n",
       "      <td>There I was. Probably the only one in the movi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13762</th>\n",
       "      <td>0</td>\n",
       "      <td>As a lord of the rings fan, I find this audio ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                         reviewText  score\n",
       "15978           0  Take a person who prides herself on having sex...      0\n",
       "3036            0  I always had a vague idea as to what the Klond...      1\n",
       "66996           0  I just couldn't do it. We bought this book bas...      0\n",
       "17660           0  There I was. Probably the only one in the movi...      1\n",
       "13762           0  As a lord of the rings fan, I find this audio ...      0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train['reviewText'].values\n",
    "train_label = train['score'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['At 23 life can be difficult for anyone, but for Jacob Jankowski, a veterinary student at Cornell, difficult is a bit of an understatement. Not only has he just tragically lost both of his parents, but thanks to the bank he is now broke, and homeless. Unclear as to what to do next Jacob starts walking, unfortunately a few hours later he realizes he is lost and his feet are covered in blisters. In short... he\\'s no better off than when he started, but... all of that is about to change when he hears a train is coming his way.  With no clue as to where he is, or where to go next, Jacob makes a decision; hop the train and see where it takes him, little did he know that that would be the circus. Finding himself knee deep in the Benzini Brother\\'s most spectacular show on Earth; Jacob quickly discovers that his \"previous\" life may (in fact) do him justice in his \"new\" one, and takes on a job as the circus\\' official veterinarian. With animals however, comes their performers and that\\'s where things start to get sticky. Marlena is beautiful, kind, smart, and more importantly... taken. Will Jacob be able to control his feelings for the one person he is forbidden from caring about? Why and how are people disappearing off of the train, and what REALLY happened that fateful day when the world flipped upside down and all hell broke loose?Gruen is very clearly a masterful storyteller.  Not only did she deliver a gasp worthy love story, but the background in which her story took place was so artfully crafted that after only 1 chapter I found it impossible to put down.  The detail involved in creating the grit of a traveling circus (while at the same time tacking such issues as the Great Depression and Prohibition) was genius, and to do it through the eyes of 1 man at very different times in his life (23 and 90 or 93) added a depth and emotional connection that would have otherwise been lost if penned differently. The story as a whole was perfectly plotted, leaving zero downtime for the readers mind to wonder off or lose focus of the moral dilemmas at hand, and at the end of the day became: just another one of those books that sucks you in and doesn\\'t let you go.Overall...a wonderful book and an interesting peek into classic American history.Happy Reading my fellow Kindle-ites and remember: animals may not be able to talk to you, but they can understand you. Be nice',\n",
       "       'was a good read ,but expected morehave read other books by F Scott Fitzgerald that I liked better.Was let down.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text[3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_text:\n",
    "    try:\n",
    "        a = len(x)\n",
    "    except:\n",
    "        print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer1 = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5900"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer1.vocab['mississippi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'cl', '##s', ']', 'after', 'stealing', 'em', '##bed', '##ding', '##s', 'from', 'the', 'bank', \"'\", 's', 'accounts', ',', 'the', 'bank', 'robber', 'was', 'seen', 'driving', 'on', 'the', 'mississippi', 'river', 'bank', 'in', 'mini', '-', 'van', '.', '[', 'sep', ']']\n"
     ]
    }
   ],
   "source": [
    "text = \"After stealing embeddings from the bank's accounts, the bank robber was seen driving on the Mississippi river bank in mini-van.\"\n",
    "text = '[CLS]' + text + '[SEP]'\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print (tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-10-17 15:35:09--  https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.17.203\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.17.203|:443... ^C\n"
     ]
    }
   ],
   "source": [
    "! wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vika/embedding_vs_bert\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "loading vocabulary file bert-base-uncased-vocab.txt\n",
      "resolved voc file bert-base-uncased-vocab.txt\n",
      "['[UNK]', 'stealing', 'em', '##bed', '##ding', '##s', 'from', 'the', 'bank', \"##'\", '##s', 'accounts', '##,', 'the', 'bank', 'robber', 'was', 'seen', 'driving', 'on', 'the', '[UNK]', 'river', 'bank', 'in', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tokenizer_custom_bert\n",
    "\n",
    "text = \"After stealing embeddings from the bank's accounts, the bank robber was seen driving on the Mississippi river bank in mini-van.\"\n",
    "text = '[CLS]' + text + '[SEP]'\n",
    "tokenizer = tokenizer_custom_bert.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print (tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['[UNK]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note можно заметить, что до ~1500 в словаре идут unused "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[unused495]',\n",
       " '[unused496]',\n",
       " '[unused497]',\n",
       " '[unused498]',\n",
       " '[unused499]',\n",
       " '[unused500]',\n",
       " '[unused501]',\n",
       " '[unused502]',\n",
       " '[unused503]',\n",
       " '[unused504]',\n",
       " '[unused505]',\n",
       " '[unused506]',\n",
       " '[unused507]',\n",
       " '[unused508]',\n",
       " '[unused509]',\n",
       " '[unused510]',\n",
       " '[unused511]',\n",
       " '[unused512]',\n",
       " '[unused513]',\n",
       " '[unused514]']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.vocab.keys())[500:520]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    39996.000000\n",
       "mean       928.194019\n",
       "std       1091.717758\n",
       "min          3.000000\n",
       "25%        244.000000\n",
       "50%        555.000000\n",
       "75%       1192.000000\n",
       "max      20986.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "reviews_len = [len(x) for x in train_text]\n",
    "pd.Series(reviews_len).hist()\n",
    "plt.show()\n",
    "pd.Series(reviews_len).describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Загрузка модели\n",
    "Изначальная модель сохранена в tf, причем сохранена очень криво"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./PretrainedBert/uncased_L-12_H-768_A-12/bert_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph('./PretrainedBert/uncased_L-12_H-768_A-12/bert_model.ckpt.meta')\n",
    "    saver.restore(sess, \"./PretrainedBert/uncased_L-12_H-768_A-12/bert_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_pretrained = \"./PretrainedBert/uncased_L-12_H-768_A-12\"\n",
    "bert_model = BertModel.from_pretrained(path_to_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segment_ids = torch.tensor(np.zeros(tokens_tensor.shape)).to(torch.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_layers, _ = bert_model(tokens_tensor, segment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 36, 768])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_layers[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 36])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "def batch_to_ids(batch):\n",
    "    batch = [text[:500] for text in batch]\n",
    "    batch = [\"[CLS] \" + text + \" [SEP]\" for text in batch]\n",
    "    tokenized_texts = [tokenizer.tokenize(sent) for sent in batch]\n",
    "    MAX_LEN = np.max(np.array([len(seq) for seq in tokenized_texts]))\n",
    "    MAX_LEN_IN_BATCH = np.max(np.array([len(seq) for seq in batch]))\n",
    "\n",
    "    #print (\"word_indexes, max len, max len in batch\", MAX_LEN, MAX_LEN_IN_BATCH)\n",
    "    input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts], maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    tokens_tensor = torch.tensor(input_ids)\n",
    "    segments_tensor = torch.tensor(np.zeros(input_ids.shape)).to(torch.int64)\n",
    "\n",
    "    return tokens_tensor, segments_tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  101,   100, 11065,  7861,  8270,  4667,  2015,  2013,  1996,  2924,\n",
       "          29618,  2015,  6115, 29623,  1996,  2924, 27307,  2001,  2464,  4439,\n",
       "           2006,  1996,   100,  2314,  2924,  1999,  7163, 29624,  6212, 29625,\n",
       "            102],\n",
       "         [  101,   100,  1996,  2173,  2005,  1037,   100,  1996,   100,   100,\n",
       "           2002,  5565,  2010,  3626,  2007,  2729,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0]]),\n",
       " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_to_ids([\"After stealing embeddings from the bank's accounts, the bank robber was seen driving on the Mississippi river bank in mini-van.\", 'Just the place for a Snark! the Bellman cried,As he landed his crew with care'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "class BertEmbedding(nn.Module):\n",
    "    \"\"\"Abstract base class for all type of layers.\"\"\"\n",
    "    def __init__(self, gpu):\n",
    "        super(BertEmbedding, self).__init__()\n",
    "        self.gpu = gpu\n",
    "        \n",
    "    def forward(self, word_sequences):\n",
    "        tokens_tensor, segments_tensor = batch_to_ids(word_sequences)\n",
    "        tokens_tensor = self.to_gpu(tokens_tensor)\n",
    "        segments_tensor = self.to_gpu(segments_tensor)\n",
    "\n",
    "               \n",
    "        #print (\"forward: token_tensor shape\", tokens_tensor.shape)\n",
    "        #print (\"forward: number_word_in_seq shape\", number_word_in_seq.shape)\n",
    "        encoded_layers, _ = bert_model(tokens_tensor, segments_tensor)\n",
    "        \n",
    "        batch_embeddings = []\n",
    "        for batch_i in range(tokens_tensor.shape[0]): #batch_size\n",
    "            token_embeddings = []\n",
    "            for token_i in range(tokens_tensor.shape[1]):  #number of token in batch element\n",
    "                hidden_layers = [] \n",
    "                for layer_i in range(len(encoded_layers)):\n",
    "                    vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "                    hidden_layers.append(vec)\n",
    "                token_embeddings.append(hidden_layers)\n",
    "            summed_last_4_layers = [torch.sum(torch.stack(layer)[-4:], 0) for layer in token_embeddings]\n",
    "            summed_last_4_layers = torch.stack(summed_last_4_layers)\n",
    "            batch_embeddings.append(summed_last_4_layers)\n",
    "        \n",
    "        answer = torch.stack(batch_embeddings)  \n",
    "        return answer\n",
    "        \n",
    "\n",
    "    def to_gpu(self, tensor):\n",
    "        if self.gpu > -1:\n",
    "            return tensor.cuda(device=self.gpu)\n",
    "        else:\n",
    "            return tensor.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_embeddings = BertEmbedding(-1)\n",
    "embeds_out =custom_embeddings([\"After stealing embeddings from the bank's accounts, the bank robber was seen driving on the Mississippi river bank in mini-van.\", 'Just the place for a Snark! the Bellman cried,As he landed his crew with care'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 31, 768])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM layer output shape torch.Size([2, 31, 256])\n",
      "LSTM layer output  tensor([[[ 1.2403e-02, -2.1831e-01,  2.1863e-02,  ..., -1.0688e-02,\n",
      "          -1.9930e-02,  1.3350e-01],\n",
      "         [-1.2861e-03, -2.6605e-02, -3.4227e-01,  ..., -3.4917e-02,\n",
      "          -2.5498e-01,  4.8522e-01],\n",
      "         [ 1.4371e-02, -5.0095e-03, -5.6394e-01,  ..., -8.4949e-02,\n",
      "          -9.7448e-03,  1.6931e-02],\n",
      "         ...,\n",
      "         [-4.5546e-03, -3.1821e-01,  7.6104e-02,  ...,  6.8097e-02,\n",
      "           4.7009e-03, -1.6586e-01],\n",
      "         [-1.5355e-02, -5.8197e-01, -6.3129e-02,  ..., -8.2794e-02,\n",
      "          -3.3687e-02,  5.1642e-02],\n",
      "         [-3.0231e-01, -2.2184e-01,  1.0633e-02,  ..., -1.5395e-01,\n",
      "           1.3210e-01,  7.8849e-02]],\n",
      "\n",
      "        [[ 2.7731e-02, -1.8341e-01,  3.9500e-01,  ..., -1.1194e-01,\n",
      "          -1.7266e-02,  2.9907e-01],\n",
      "         [ 3.8574e-04, -2.3737e-02, -6.6040e-02,  ..., -6.7338e-03,\n",
      "          -8.7242e-03,  4.2282e-01],\n",
      "         [-2.2248e-04,  4.7318e-02, -7.2799e-01,  ...,  1.0580e-01,\n",
      "          -1.0329e-04,  3.4869e-01],\n",
      "         ...,\n",
      "         [-9.9886e-03,  2.1085e-02, -3.9533e-02,  ...,  2.8217e-02,\n",
      "           1.4058e-03, -4.8141e-01],\n",
      "         [ 2.1725e-04,  1.1956e-02, -2.7112e-01,  ..., -8.5538e-02,\n",
      "           1.3836e-03, -3.8650e-01],\n",
      "         [ 1.1630e-02,  1.2246e-01, -3.0127e-02,  ...,  6.7376e-03,\n",
      "          -9.5243e-04, -2.9899e-01]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hidden=None\n",
    "lstm = nn.LSTM(input_size=768, hidden_size=256, num_layers=1, batch_first=True)\n",
    "lstm_out, h = lstm(embeds_out, hidden)\n",
    "print ('LSTM layer output shape', lstm_out.shape)\n",
    "print ('LSTM layer output ', lstm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, output_size, batch_size, cuda):\n",
    "\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.gpu = cuda\n",
    "        self.embedding_dim = 768\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding = BertEmbedding(self.gpu)\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=hidden_dim, bidirectional=True, num_layers=1, batch_first=True)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, output_size)\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # first is the hidden h\n",
    "        # second is the cell c\n",
    "        if self.gpu > -1:\n",
    "            return (Variable(torch.zeros(2, batch_size, self.hidden_dim).cuda()),\n",
    "                    Variable(torch.zeros(2, batch_size, self.hidden_dim).cuda()))\n",
    "        else:\n",
    "            return (Variable(torch.zeros(2, batch_size, self.hidden_dim)),\n",
    "                    Variable(torch.zeros(2, batch_size, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        x = self.embedding(sentence)\n",
    "        lstm_out, (ht, ct) = self.lstm(x, self.hidden)\n",
    "        y = self.hidden2label(ht[-1])\n",
    "        log_probs = F.log_softmax(y, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cuda0 = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.to(device=cuda0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 2\n",
    "hidden_dim = 256\n",
    "batch_size = 20\n",
    "\n",
    "model = LSTMClassifier(hidden_dim, output_size, 20, cuda = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(train_text[7:27])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 2])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 117, 768])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer output shape torch.Size([3, 83, 768])\n",
      "Embedding layer output  tensor([[[-4.4146e-01, -3.1786e+00, -1.6374e-01,  ..., -3.7876e-01,\n",
      "           5.9078e-01,  7.4074e-01],\n",
      "         [-1.5193e-01,  4.2343e+00,  5.2023e-01,  ...,  1.0053e+00,\n",
      "           6.3401e-01,  1.3098e+00],\n",
      "         [ 2.1064e+00,  2.4128e+00,  1.3109e+00,  ..., -4.1954e-02,\n",
      "          -1.5611e-02,  1.1473e+00],\n",
      "         ...,\n",
      "         [-2.9670e+00, -1.0039e+00,  2.8182e+00,  ..., -1.3264e+00,\n",
      "          -2.7967e+00, -2.0073e+00],\n",
      "         [-1.1177e+00, -3.6020e+00, -9.1324e-01,  ...,  1.1221e+00,\n",
      "           2.5948e-01,  5.7704e-01],\n",
      "         [ 9.9009e-01,  4.8730e-01, -1.0148e-01,  ...,  2.9960e-01,\n",
      "          -2.4175e-01,  1.9994e-02]],\n",
      "\n",
      "        [[-3.0533e-01, -1.0331e-01,  3.3248e+00,  ..., -8.8722e-01,\n",
      "           1.4878e-01, -2.6120e+00],\n",
      "         [-7.5062e-03,  1.9412e+00,  4.8105e+00,  ...,  1.8072e+00,\n",
      "          -1.8479e+00,  1.2648e+00],\n",
      "         [-1.7691e+00,  1.8795e+00,  8.8901e-01,  ..., -6.1095e-01,\n",
      "           9.7130e-01,  1.7724e-01],\n",
      "         ...,\n",
      "         [ 2.1850e+00, -8.9189e-01,  1.3052e+00,  ..., -2.9245e+00,\n",
      "           4.5344e-02, -5.7088e+00],\n",
      "         [ 1.9262e+00,  1.5765e-01,  3.3185e+00,  ..., -1.5766e+00,\n",
      "           1.7301e+00, -4.3600e+00],\n",
      "         [ 1.4280e+00, -1.5311e-01,  4.9852e+00,  ..., -3.6097e+00,\n",
      "           2.3125e-01, -5.8877e+00]],\n",
      "\n",
      "        [[-1.7032e-01,  2.5879e-01,  2.7376e+00,  ..., -1.2716e+00,\n",
      "          -3.2940e-02, -4.2690e-01],\n",
      "         [-2.4410e+00,  2.3424e+00,  2.3409e+00,  ..., -8.8289e-02,\n",
      "          -2.2337e-01, -7.5541e-01],\n",
      "         [ 3.3570e-01,  4.9989e+00,  6.1102e+00,  ...,  2.2957e-01,\n",
      "          -2.1936e+00,  2.2100e+00],\n",
      "         ...,\n",
      "         [ 8.4560e-01, -1.5489e+00,  3.7736e+00,  ..., -1.6698e+00,\n",
      "           8.1475e-01, -3.7511e+00],\n",
      "         [-1.5124e-01, -5.1156e-01,  2.8883e+00,  ..., -2.5449e+00,\n",
      "           1.8446e+00, -4.0089e+00],\n",
      "         [ 1.0708e+00,  2.9765e+00,  2.3810e+00,  ..., -2.3388e+00,\n",
      "           4.4508e-01, -4.9333e+00]]], grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "embeds_out =custom_embeddings(train_text[4:7])\n",
    "print ('Embedding layer output shape', embeds_out.shape)\n",
    "print ('Embedding layer output ', embeds_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 2])\n",
      "torch.Size([20, 2])\n",
      "torch.Size([20, 2])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-308-48854cc5ad49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "it = 0\n",
    "tot = 0\n",
    "act = 0\n",
    "\n",
    "PRINT_EVERY = 10\n",
    "\n",
    "from tqdm import tqdm\n",
    "loss_function = nn.NLLLoss()\n",
    "for i in range(len(train_label)//batch_size):\n",
    "    idx = [i*batch_size + k for k in range(batch_size)]\n",
    "    batch = train_text[idx]\n",
    "    y = train_label[idx]\n",
    "    y = torch.from_numpy(np.array((y)))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(batch)\n",
    "    print (pred.shape)\n",
    "    l = loss_function(pred, y)\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    it += 1\n",
    "    tot += l.item()\n",
    "    act += (pred.detach().numpy().argmax() == y)\n",
    "    if it == PRINT_EVERY:\n",
    "        print(tot/it, act/it)\n",
    "        it = 0\n",
    "        tot = 0\n",
    "        act = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "simple_classification_problem.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkKoSz6pqBvY",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foN6sLgsqBvb",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxvvZQX3qBvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Пожалуйста, выполните код ниже, дабы скачать в коллаб нужные файлы.\n",
        "#Если вы не работаете в коллабе, выполнять код не нужно\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vjd6QspNqBvg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 965
        },
        "outputId": "3e2e82de-b77c-4c84-e54d-9dfe421755de"
      },
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Y4xJcBaw5ub4cfCtYIdLbgPvFGvHyTsD' -O tokenizer_custom_bert.py\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1c7X35380Ny6r9ZUaKzoBL8cfVYSw1sq5' -O file_utils_custom.py\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1hD1zTPdY441xiXeTQDYeVR-7P8laLgsE' -O review_books.tsv"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-17 21:52:17--  https://docs.google.com/uc?export=download&id=1Y4xJcBaw5ub4cfCtYIdLbgPvFGvHyTsD\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.120.102, 108.177.120.113, 108.177.120.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.120.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-00-6s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/2j7du5jsij13rh3ttovbreltt6mg18g4/1571342400000/01961971800886548445/*/1Y4xJcBaw5ub4cfCtYIdLbgPvFGvHyTsD?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2019-10-17 21:52:17--  https://doc-00-6s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/2j7du5jsij13rh3ttovbreltt6mg18g4/1571342400000/01961971800886548445/*/1Y4xJcBaw5ub4cfCtYIdLbgPvFGvHyTsD?e=download\n",
            "Resolving doc-00-6s-docs.googleusercontent.com (doc-00-6s-docs.googleusercontent.com)... 108.177.111.132, 2607:f8b0:4001:c07::84\n",
            "Connecting to doc-00-6s-docs.googleusercontent.com (doc-00-6s-docs.googleusercontent.com)|108.177.111.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19193 (19K) [text/x-python]\n",
            "Saving to: ‘tokenizer_custom_bert.py’\n",
            "\n",
            "\rtokenizer_custom_be   0%[                    ]       0  --.-KB/s               \rtokenizer_custom_be 100%[===================>]  18.74K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-10-17 21:52:17 (117 MB/s) - ‘tokenizer_custom_bert.py’ saved [19193/19193]\n",
            "\n",
            "--2019-10-17 21:52:20--  https://docs.google.com/uc?export=download&id=1c7X35380Ny6r9ZUaKzoBL8cfVYSw1sq5\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.120.102, 108.177.120.101, 108.177.120.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.120.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-08-6s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/qissdm85b3papj8dge5d33tsca8cki53/1571342400000/01961971800886548445/*/1c7X35380Ny6r9ZUaKzoBL8cfVYSw1sq5?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2019-10-17 21:52:20--  https://doc-08-6s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/qissdm85b3papj8dge5d33tsca8cki53/1571342400000/01961971800886548445/*/1c7X35380Ny6r9ZUaKzoBL8cfVYSw1sq5?e=download\n",
            "Resolving doc-08-6s-docs.googleusercontent.com (doc-08-6s-docs.googleusercontent.com)... 108.177.111.132, 2607:f8b0:4001:c07::84\n",
            "Connecting to doc-08-6s-docs.googleusercontent.com (doc-08-6s-docs.googleusercontent.com)|108.177.111.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7431 (7.3K) [text/x-python]\n",
            "Saving to: ‘file_utils_custom.py’\n",
            "\n",
            "file_utils_custom.p 100%[===================>]   7.26K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-10-17 21:52:31 (81.6 MB/s) - ‘file_utils_custom.py’ saved [7431/7431]\n",
            "\n",
            "--2019-10-17 21:52:32--  https://docs.google.com/uc?export=download&id=1hD1zTPdY441xiXeTQDYeVR-7P8laLgsE\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.120.138, 108.177.120.139, 108.177.120.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.120.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0c-6s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/9lcd2o1s8vb4he725spoirbatklalfu6/1571342400000/01961971800886548445/*/1hD1zTPdY441xiXeTQDYeVR-7P8laLgsE?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2019-10-17 21:52:35--  https://doc-0c-6s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/9lcd2o1s8vb4he725spoirbatklalfu6/1571342400000/01961971800886548445/*/1hD1zTPdY441xiXeTQDYeVR-7P8laLgsE?e=download\n",
            "Resolving doc-0c-6s-docs.googleusercontent.com (doc-0c-6s-docs.googleusercontent.com)... 108.177.111.132, 2607:f8b0:4001:c07::84\n",
            "Connecting to doc-0c-6s-docs.googleusercontent.com (doc-0c-6s-docs.googleusercontent.com)|108.177.111.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/tab-separated-values]\n",
            "Saving to: ‘review_books.tsv’\n",
            "\n",
            "review_books.tsv        [          <=>       ]  84.86M  36.3MB/s    in 2.3s    \n",
            "\n",
            "2019-10-17 21:52:48 (36.3 MB/s) - ‘review_books.tsv’ saved [88986307]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQKfapWHtVZs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "b843469b-951a-40d8-969d-c63e04a365e0"
      },
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1ePm7_OV1TBNPgPBDmSW14iFBPZaWAShV' -O uncased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-17 21:54:02--  https://docs.google.com/uc?export=download&id=1ePm7_OV1TBNPgPBDmSW14iFBPZaWAShV\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.120.102, 108.177.120.113, 108.177.120.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.120.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip’\n",
            "\n",
            "\r          uncased_L     [<=>                 ]       0  --.-KB/s               \runcased_L-12_H-768_     [ <=>                ]   3.19K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-10-17 21:54:02 (37.1 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [3263]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZaAdqlpqBvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "df = pd.read_csv('review_books.tsv', sep='\\t') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Vl19rbNqBvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos = df[df['score'] == 1]\n",
        "neg = df[df['score'] == 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aNyOjNsqBvt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1cd8a462-a9cf-4687-a548-0d033f239d8e"
      },
      "source": [
        "print (len(pos), len(neg), len(df))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "78994 21008 100002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdPFtHiHqBvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos = pos[:21000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwnIEddmqBv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_train = pos[:20000].dropna()\n",
        "pos_test = pos[21000:].dropna()\n",
        "neg_train = neg[:20000].dropna()\n",
        "neg_test = neg[21000:].dropna()\n",
        "\n",
        "train = pd.concat([pos_train, neg_train])\n",
        "train = train.reindex(np.random.permutation(train.index))\n",
        "test = pd.concat([pos_test, neg_test])\n",
        "test = test.reindex(np.random.permutation(test.index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQeeY7TgqBv4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "9399f32e-a9aa-434f-ced5-1ee4d606c4ee"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>36800</th>\n",
              "      <td>0</td>\n",
              "      <td>I was a bit disappointed with this book and ha...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7869</th>\n",
              "      <td>0</td>\n",
              "      <td>This is a book about lonely planet travellers ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66976</th>\n",
              "      <td>0</td>\n",
              "      <td>If your lover insists you read this, run as fa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37710</th>\n",
              "      <td>0</td>\n",
              "      <td>I loved the first two books, this one was disa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89990</th>\n",
              "      <td>0</td>\n",
              "      <td>I consider myself a \"wordie\" - love to hear ab...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0                                         reviewText  score\n",
              "36800           0  I was a bit disappointed with this book and ha...      0\n",
              "7869            0  This is a book about lonely planet travellers ...      1\n",
              "66976           0  If your lover insists you read this, run as fa...      0\n",
              "37710           0  I loved the first two books, this one was disa...      0\n",
              "89990           0  I consider myself a \"wordie\" - love to hear ab...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGmB9IgTqBv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_text = train['reviewText'].values\n",
        "train_label = train['score'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyZtb7LSqBwA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "19effbdd-c0a2-4c45-dbe4-0d8fb2b000a8"
      },
      "source": [
        "train_text[3:5]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([\"I loved the first two books, this one was disappointing.  Definitely not the ending that you expect. This one doesn't hold you in utter suspense either.  You kind of see what's coming and it loses the excitement that the first two books had.  Not a horrible read, but not in comparison to divergent and insurgent.  :(\",\n",
              "       'I consider myself a \"wordie\" - love to hear about interesting word origins and the like - and so was very happy to see this book at the used book store.  After reading it, however, it is going back to the used book store.  Parts of it were okay, but nothing was especially earth-shattering about it.  And, most unfortunately, so many of the entries were expressions that only my grandfather remembers anymore that it was more or less irrelevant to me.  I suppose there\\'s a quirk factor to the book, and perhaps it would be useful for someone wanting to write a book set in the 40\\'s or 50\\'s, but there are many better books to buy if words are your thing.'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5y1zgip3uCg1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "outputId": "15174579-2732-4863-9044-7b630e770961"
      },
      "source": [
        "!pip3 install pytorch-pretrained-bert"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.16.5)\n",
            "Collecting regex (from pytorch-pretrained-bert)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/60/d9782c56ceefa76033a00e1f84cd8c586c75e6e7fea2cd45ee8b46a386c5/regex-2019.08.19-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 11.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.9.243)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.243 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.12.243)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.243->boto3->pytorch-pretrained-bert) (2.5.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.243->boto3->pytorch-pretrained-bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.243->boto3->pytorch-pretrained-bert) (1.12.0)\n",
            "Installing collected packages: regex, pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2 regex-2019.8.19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-MLkKzwqBwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC8_hPwsqBwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer1 = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrMFF0MRqBwQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ceb90276-7c7d-41db-8021-53a022e5d863"
      },
      "source": [
        "tokenizer1.vocab['mississippi']"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5900"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWr4OF8BqBwU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "08f86ec6-f7e9-4f33-c58d-a4cdbaa6ba0b"
      },
      "source": [
        "text = \"After stealing embeddings from the bank's accounts, the bank robber was seen driving on the Mississippi river bank in mini-van.\"\n",
        "text = '[CLS]' + text + '[SEP]'\n",
        "tokenized_text = tokenizer1.tokenize(text)\n",
        "print (tokenized_text)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[', 'cl', '##s', ']', 'after', 'stealing', 'em', '##bed', '##ding', '##s', 'from', 'the', 'bank', \"'\", 's', 'accounts', ',', 'the', 'bank', 'robber', 'was', 'seen', 'driving', 'on', 'the', 'mississippi', 'river', 'bank', 'in', 'mini', '-', 'van', '.', '[', 'sep', ']']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMGLh0P8qBwX",
        "colab_type": "code",
        "colab": {},
        "outputId": "5a3e42e2-a8b4-48a9-fb39-a23c5b526d48"
      },
      "source": [
        "! wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-17 15:35:09--  https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.17.203\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.17.203|:443... ^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEjdRps4qBwb",
        "colab_type": "code",
        "colab": {},
        "outputId": "4d661db9-5bf6-4011-d5c4-546924b6c7ea"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/vika/embedding_vs_bert\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Od83a4ehqBwe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "d47971c9-fb8e-4f7d-e988-8cbab080ca52"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import tokenizer_custom_bert\n",
        "\n",
        "text = \"After stealing embeddings from the bank's accounts, the bank robber was seen driving on the Mississippi river bank in mini-van.\"\n",
        "text = '[CLS]' + text + '[SEP]'\n",
        "tokenizer = tokenizer_custom_bert.BertTokenizer.from_pretrained(\"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\")\n",
        "tokenized_text = tokenizer.tokenize(text)\n",
        "print (tokenized_text)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "10/17/2019 22:12:40 - INFO - file_utils_custom -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache, downloading to /tmp/tmpkpvrg3b1\n",
            "100%|██████████| 231508/231508 [00:00<00:00, 2620766.44B/s]\n",
            "10/17/2019 22:12:40 - INFO - file_utils_custom -   copying /tmp/tmpkpvrg3b1 to cache at /home/vika/targer/pretrained/uncased_L-12_H-768_A-12/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "10/17/2019 22:12:41 - INFO - file_utils_custom -   creating metadata file for /home/vika/targer/pretrained/uncased_L-12_H-768_A-12/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "10/17/2019 22:12:41 - INFO - file_utils_custom -   removing temp file /tmp/tmpkpvrg3b1\n",
            "10/17/2019 22:12:41 - INFO - tokenizer_custom_bert -   loading vocabulary file %s from cache at %s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "resolved voc file /home/vika/targer/pretrained/uncased_L-12_H-768_A-12/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "WARNING:tensorflow:From /content/tokenizer_custom_bert.py:187: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "10/17/2019 22:12:41 - WARNING - tensorflow -   From /content/tokenizer_custom_bert.py:187: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['[UNK]', 'stealing', 'em', '##bed', '##ding', '##s', 'from', 'the', 'bank', \"##'\", '##s', 'accounts', '##,', 'the', 'bank', 'robber', 'was', 'seen', 'driving', 'on', 'the', '[UNK]', 'river', 'bank', 'in', '[UNK]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sI2tC_nQqBwh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2402f566-002f-429d-c5c9-5f7065c48a91"
      },
      "source": [
        "tokenizer.vocab['[UNK]']"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvgZvJBWqBwx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "672b5265-70d9-4c46-d6fe-6a49c511dc53"
      },
      "source": [
        "list(tokenizer.vocab.keys())[500:520]"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[unused495]',\n",
              " '[unused496]',\n",
              " '[unused497]',\n",
              " '[unused498]',\n",
              " '[unused499]',\n",
              " '[unused500]',\n",
              " '[unused501]',\n",
              " '[unused502]',\n",
              " '[unused503]',\n",
              " '[unused504]',\n",
              " '[unused505]',\n",
              " '[unused506]',\n",
              " '[unused507]',\n",
              " '[unused508]',\n",
              " '[unused509]',\n",
              " '[unused510]',\n",
              " '[unused511]',\n",
              " '[unused512]',\n",
              " '[unused513]',\n",
              " '[unused514]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0Wyzf4WqBw1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "0ffda4ef-ec90-4276-9e59-25a06c8e3d9b"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "reviews_len = [len(x) for x in train_text]\n",
        "pd.Series(reviews_len).hist()\n",
        "plt.show()\n",
        "pd.Series(reviews_len).describe()\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJHUYDpKkDsNBktRhOEiSOv4fhCIHOPpS/tYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    39996.000000\n",
              "mean       928.194019\n",
              "std       1091.717758\n",
              "min          3.000000\n",
              "25%        244.000000\n",
              "50%        555.000000\n",
              "75%       1192.000000\n",
              "max      20986.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH_oeQ-CzQKu",
        "colab_type": "text"
      },
      "source": [
        "## Загрузка модели\n",
        "\n",
        "Все претренированные модели лежат тут: https://github.com/google-research/bert\n",
        "\n",
        "Мы будем работать с base, uncased - меленьким бертом, обученном на lower-case текстах.\n",
        "\n",
        "Изначальная модель сохранена в tf, причем сохранена очень криво.\n",
        "\n",
        "Для того, чтобы восстановить pytorch-модель, нужен config.json, *.ckpt файл. В скачанной папке нет *.ckpt в чистом виде, его надо создать через метаграф."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd67yFY3yuWD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "37c246c5-c005-41e2-cdef-05af9dd01fc4"
      },
      "source": [
        "!wget 'https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip'"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-17 22:19:37--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.183.128, 2607:f8b0:4001:c07::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.183.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip.1’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M   110MB/s    in 3.5s    \n",
            "\n",
            "2019-10-17 22:19:41 (110 MB/s) - ‘uncased_L-12_H-768_A-12.zip.1’ saved [407727028/407727028]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq-Qyp0XyGaQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "170be082-512e-4d84-c60b-d9b1d41317fb"
      },
      "source": [
        "! unzip 'uncased_L-12_H-768_A-12.zip'"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  uncased_L-12_H-768_A-12.zip\n",
            "   creating: uncased_L-12_H-768_A-12/\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Coy6EiZ1qBw8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0d8b6fa7-b5d0-4025-d7fb-c57048deb112"
      },
      "source": [
        "import tensorflow as tf\n",
        "with tf.Session() as sess:\n",
        "    saver = tf.train.import_meta_graph('./uncased_L-12_H-768_A-12/bert_model.ckpt.meta')\n",
        "    saver.restore(sess, \"./uncased_L-12_H-768_A-12/bert_model.ckpt\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./uncased_L-12_H-768_A-12/bert_model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "10/17/2019 22:21:52 - INFO - tensorflow -   Restoring parameters from ./uncased_L-12_H-768_A-12/bert_model.ckpt\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82Z3gIji0SxF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "outputId": "1dff25a6-f7ce-4161-9d14-2d1720b08822"
      },
      "source": [
        "!pip3 install pytorch_transformers"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 3.4MB/s \n",
            "\u001b[?25hCollecting sacremoses (from pytorch_transformers)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 44.6MB/s \n",
            "\u001b[?25hCollecting sentencepiece (from pytorch_transformers)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 43.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.9.243)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (4.28.1)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2019.8.19)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.16.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (0.14.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.243 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (1.12.243)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.2.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (3.0.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.243->boto3->pytorch_transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.243->boto3->pytorch_transformers) (2.5.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=f3400ad8123e307b74a3cb62a35f8bad659365cea3da3174574b0cd6048204bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, pytorch-transformers\n",
            "Successfully installed pytorch-transformers-1.2.0 sacremoses-0.0.35 sentencepiece-0.1.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNrniy0y0N7Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e32b8f51-f4fd-4129-9e03-ca6fe4842b1a"
      },
      "source": [
        "import torch\n",
        "\n",
        "from pytorch_transformers.modeling_bert import BertConfig, BertForPreTraining, load_tf_weights_in_bert\n",
        "\n",
        "\n",
        "tf_checkpoint_path=\"./uncased_L-12_H-768_A-12/bert_model.ckpt\"\n",
        "bert_config_file = \"./uncased_L-12_H-768_A-12/bert_config.json\"\n",
        "pytorch_dump_path=\"./uncased_L-12_H-768_A-12/pytorch_model\"\n",
        "\n",
        "config = BertConfig.from_json_file(bert_config_file)\n",
        "print(\"Building PyTorch model from configuration: {}\".format(str(config)))\n",
        "model = BertForPreTraining(config)\n",
        "\n",
        "# Load weights from tf checkpoint\n",
        "load_tf_weights_in_bert(model, config, tf_checkpoint_path)\n",
        "\n",
        "# Save pytorch-model\n",
        "print(\"Save PyTorch model to {}\".format(pytorch_dump_path))\n",
        "torch.save(model.state_dict(), pytorch_dump_path)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building PyTorch model from configuration: {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Converting TensorFlow checkpoint from /content/uncased_L-12_H-768_A-12/bert_model.ckpt\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/embeddings/LayerNorm/beta with shape [768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/embeddings/LayerNorm/gamma with shape [768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/embeddings/position_embeddings with shape [512, 768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/embeddings/word_embeddings with shape [30522, 768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [3072]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [768, 3072]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [3072, 768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [3072]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [768, 3072]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [3072, 768]\n",
            "10/17/2019 22:25:22 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [3072]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [768, 3072]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_10/output/dense/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_10/output/dense/kernel with shape [3072, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [3072]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [768, 3072]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_11/output/dense/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_11/output/dense/kernel with shape [3072, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [3072]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [768, 3072]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [3072, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [3072]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [768, 3072]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [3072, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [3072]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [768, 3072]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_4/output/dense/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_4/output/dense/kernel with shape [3072, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [3072]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [768, 3072]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_5/output/dense/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_5/output/dense/kernel with shape [3072, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [3072]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [768, 3072]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_6/output/dense/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_6/output/dense/kernel with shape [3072, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [3072]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [768, 3072]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_7/output/dense/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_7/output/dense/kernel with shape [3072, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [3072]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [768, 3072]\n",
            "10/17/2019 22:25:23 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [768]\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [768]\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_8/output/dense/bias with shape [768]\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_8/output/dense/kernel with shape [3072, 768]\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [768]\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [768]\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [768]\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [768]\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [768]\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [768]\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [3072]\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [768, 3072]\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [768]\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_9/output/dense/bias with shape [768]\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/pooler/dense/bias with shape [768]\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight bert/pooler/dense/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight cls/predictions/output_bias with shape [30522]\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight cls/predictions/transform/LayerNorm/beta with shape [768]\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight cls/predictions/transform/LayerNorm/gamma with shape [768]\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight cls/predictions/transform/dense/bias with shape [768]\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight cls/predictions/transform/dense/kernel with shape [768, 768]\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight cls/seq_relationship/output_bias with shape [2]\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Loading TF weight cls/seq_relationship/output_weights with shape [2, 768]\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['cls', 'predictions', 'output_bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'kernel']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['cls', 'seq_relationship', 'output_bias']\n",
            "10/17/2019 22:25:24 - INFO - pytorch_transformers.modeling_bert -   Initialize PyTorch weight ['cls', 'seq_relationship', 'output_weights']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Save PyTorch model to ./uncased_L-12_H-768_A-12/pytorch_model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8JuANPCqBxA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "outputId": "9e50283f-60c2-4b15-cafd-c6c5171fa637"
      },
      "source": [
        "path_to_pretrained = \"./uncased_L-12_H-768_A-12\"\n",
        "bert_model = BertModel.from_pretrained(path_to_pretrained)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/17/2019 22:26:03 - INFO - pytorch_pretrained_bert.modeling -   loading archive file ./uncased_L-12_H-768_A-12\n",
            "10/17/2019 22:26:03 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlXxPoaU1ehB",
        "colab_type": "text"
      },
      "source": [
        "Посмотрим на модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqTgQoCA1byr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0e6a85f0-96d6-4fcd-c4bc-8a0a78c7a91a"
      },
      "source": [
        "bert_model"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): BertLayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): BertLayerNorm()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG9v3zge1kx0",
        "colab_type": "text"
      },
      "source": [
        "**!!! Важно. Bert учится и он огромен. Нужно заморозить все слои и если, разморзить несколько**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1CObMgL1kAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for param in bert_model.parameters():\n",
        "    param.requires_grad = False\n",
        "#for i in [0]: <--- сюда можете писать номера слоев, которые хотите разморозить\n",
        "    #for param in self.emb.encoder.layer[i].parameters():\n",
        "        #param.requires_grad = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6POkjx7qBxD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segment_ids = torch.tensor(np.zeros(tokens_tensor.shape)).to(torch.int64)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtrG2kpEqBxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_layers, _ = bert_model(tokens_tensor, segment_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoOwuw4BqBxK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f98edda4-6c7a-40e2-ec61-2cc09af23937"
      },
      "source": [
        "len(encoded_layers)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uj3n8dxvqBxN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f919c8fb-1ef8-4554-cf44-bd021dd7967e"
      },
      "source": [
        "encoded_layers[2].shape"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 26, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PCpR137qBxT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c9afbaaf-b5ba-471f-cae4-f627743a2c6f"
      },
      "source": [
        "tokens_tensor.shape"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 26])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO_ZC5J2qBxX",
        "colab_type": "text"
      },
      "source": [
        "## NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbSd0eXTqBxY",
        "colab_type": "text"
      },
      "source": [
        "## Embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w83ZNUQdqBxZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "def batch_to_ids(batch):\n",
        "    batch = [text[:500] for text in batch]\n",
        "    batch = [\"[CLS] \" + text + \" [SEP]\" for text in batch]\n",
        "    tokenized_texts = [tokenizer.tokenize(sent) for sent in batch]\n",
        "    MAX_LEN = np.max(np.array([len(seq) for seq in tokenized_texts]))\n",
        "    MAX_LEN_IN_BATCH = np.max(np.array([len(seq) for seq in batch]))\n",
        "\n",
        "    #print (\"word_indexes, max len, max len in batch\", MAX_LEN, MAX_LEN_IN_BATCH)\n",
        "    input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts], maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "    tokens_tensor = torch.tensor(input_ids)\n",
        "    segments_tensor = torch.tensor(np.zeros(input_ids.shape)).to(torch.int64)\n",
        "\n",
        "    return tokens_tensor, segments_tensor\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VY4gtXa7qBxd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "df5ff5e9-6ec7-4d1f-d2e4-06ac51414a02"
      },
      "source": [
        "batch_to_ids([\"After stealing embeddings from the bank's accounts, the bank robber was seen driving on the Mississippi river bank in mini-van.\", 'Just the place for a Snark! the Bellman cried,As he landed his crew with care'])"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[  101,   100, 11065,  7861,  8270,  4667,  2015,  2013,  1996,  2924,\n",
              "          29618,  2015,  6115, 29623,  1996,  2924, 27307,  2001,  2464,  4439,\n",
              "           2006,  1996,   100,  2314,  2924,  1999,  7163, 29624,  6212, 29625,\n",
              "            102],\n",
              "         [  101,   100,  1996,  2173,  2005,  1037,   100,  1996,   100,   100,\n",
              "           2002,  5565,  2010,  3626,  2007,  2729,   102,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0]]),\n",
              " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoswsC7sqBxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "class BertEmbedding(nn.Module):\n",
        "    \"\"\"Abstract base class for all type of layers.\"\"\"\n",
        "    def __init__(self, gpu):\n",
        "        super(BertEmbedding, self).__init__()\n",
        "        self.gpu = gpu\n",
        "        \n",
        "    def forward(self, word_sequences):\n",
        "        tokens_tensor, segments_tensor = batch_to_ids(word_sequences)\n",
        "        tokens_tensor = self.to_gpu(tokens_tensor)\n",
        "        segments_tensor = self.to_gpu(segments_tensor)\n",
        "\n",
        "               \n",
        "        #print (\"forward: token_tensor shape\", tokens_tensor.shape)\n",
        "        #print (\"forward: number_word_in_seq shape\", number_word_in_seq.shape)\n",
        "        encoded_layers, _ = bert_model(tokens_tensor, segments_tensor)\n",
        "        \n",
        "        batch_embeddings = []\n",
        "        for batch_i in range(tokens_tensor.shape[0]): #batch_size\n",
        "            token_embeddings = []\n",
        "            for token_i in range(tokens_tensor.shape[1]):  #number of token in batch element\n",
        "                hidden_layers = [] \n",
        "                for layer_i in range(len(encoded_layers)):\n",
        "                    vec = encoded_layers[layer_i][batch_i][token_i]\n",
        "                    hidden_layers.append(vec)\n",
        "                token_embeddings.append(hidden_layers)\n",
        "            summed_last_4_layers = [torch.sum(torch.stack(layer)[-4:], 0) for layer in token_embeddings]\n",
        "            summed_last_4_layers = torch.stack(summed_last_4_layers)\n",
        "            batch_embeddings.append(summed_last_4_layers)\n",
        "        \n",
        "        answer = torch.stack(batch_embeddings)  \n",
        "        return answer\n",
        "        \n",
        "\n",
        "    def to_gpu(self, tensor):\n",
        "        if self.gpu > -1:\n",
        "            return tensor.cuda(device=self.gpu)\n",
        "        else:\n",
        "            return tensor.cpu()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vX9-bCcqBxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "custom_embeddings = BertEmbedding(-1)\n",
        "embeds_out =custom_embeddings([\"After stealing embeddings from the bank's accounts, the bank robber was seen driving on the Mississippi river bank in mini-van.\", 'Just the place for a Snark! the Bellman cried,As he landed his crew with care'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljxkMARdqBxl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "373fc14e-467c-486b-cd9e-c78345523f89"
      },
      "source": [
        "embeds_out.shape"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 31, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoy5RGQrqBxn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "de98b078-c537-495a-95c4-454e656fd8fc"
      },
      "source": [
        "hidden=None\n",
        "lstm = nn.LSTM(input_size=768, hidden_size=256, num_layers=1, batch_first=True)\n",
        "lstm_out, h = lstm(embeds_out, hidden)\n",
        "print ('LSTM layer output shape', lstm_out.shape)\n",
        "print ('LSTM layer output ', lstm_out)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTM layer output shape torch.Size([2, 31, 256])\n",
            "LSTM layer output  tensor([[[ 0.6362,  0.0739,  0.4911,  ...,  0.0049,  0.0468, -0.3158],\n",
            "         [ 0.7823,  0.0214,  0.0075,  ...,  0.0231,  0.0571, -0.1108],\n",
            "         [ 0.2405, -0.1551, -0.2549,  ...,  0.0235,  0.0870, -0.3907],\n",
            "         ...,\n",
            "         [-0.2492,  0.5931,  0.1870,  ...,  0.0815,  0.0458, -0.0214],\n",
            "         [ 0.4238,  0.5007,  0.1860,  ...,  0.2589, -0.0578, -0.4655],\n",
            "         [ 0.1545,  0.2924, -0.0239,  ...,  0.5487, -0.1871, -0.1432]],\n",
            "\n",
            "        [[ 0.4868, -0.0038,  0.6214,  ...,  0.0319,  0.0481, -0.7090],\n",
            "         [ 0.6347, -0.0040,  0.8096,  ..., -0.1290,  0.0375, -0.1384],\n",
            "         [ 0.1156, -0.0015,  0.2906,  ...,  0.0697, -0.0019, -0.5138],\n",
            "         ...,\n",
            "         [ 0.2627,  0.8592,  0.2469,  ...,  0.7008, -0.0580, -0.1797],\n",
            "         [ 0.4174,  0.5454,  0.3148,  ...,  0.8545, -0.2055,  0.6822],\n",
            "         [ 0.5388,  0.9116,  0.2724,  ...,  0.4064, -0.1580,  0.2077]]],\n",
            "       grad_fn=<TransposeBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aflg7xh6qBxq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class LSTMClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_dim, output_size, batch_size, cuda):\n",
        "\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.gpu = cuda\n",
        "        self.embedding_dim = 768\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.embedding = BertEmbedding(self.gpu)\n",
        "        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=hidden_dim, bidirectional=True, num_layers=1, batch_first=True)\n",
        "        self.hidden2label = nn.Linear(hidden_dim, output_size)\n",
        "        self.hidden = self.init_hidden(batch_size)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # first is the hidden h\n",
        "        # second is the cell c\n",
        "        if self.gpu > -1:\n",
        "            return (Variable(torch.zeros(2, batch_size, self.hidden_dim).cuda()),\n",
        "                    Variable(torch.zeros(2, batch_size, self.hidden_dim).cuda()))\n",
        "        else:\n",
        "            return (Variable(torch.zeros(2, batch_size, self.hidden_dim)),\n",
        "                    Variable(torch.zeros(2, batch_size, self.hidden_dim)))\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        x = self.embedding(sentence)\n",
        "        lstm_out, (ht, ct) = self.lstm(x, self.hidden)\n",
        "        y = self.hidden2label(ht[-1])\n",
        "        log_probs = F.log_softmax(y, dim=1)\n",
        "        return log_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UansGLwxqBxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cuda0 = torch.device('cuda:0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2Dde9B-qBxv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.to(device=cuda0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbiLkfP_qBxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_size = 2\n",
        "hidden_dim = 256\n",
        "batch_size = 20\n",
        "\n",
        "model = LSTMClassifier(hidden_dim, output_size, 20, cuda = -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1bSjGiiqBx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = model(train_text[7:27])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y7WwhxkqBx4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1de016e9-b1cd-4a52-d0e4-4141dc51ee30"
      },
      "source": [
        "pred.shape"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnQ956NZqBx9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "da9fcc9f-d6a1-44c2-d3ec-2dfc63b50e8f"
      },
      "source": [
        "embeds_out.shape"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 31, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGUeVxjlqByB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        },
        "outputId": "63db4511-6c4d-4163-9d57-5b6418aee901"
      },
      "source": [
        "embeds_out =custom_embeddings(train_text[4:7])\n",
        "print ('Embedding layer output shape', embeds_out.shape)\n",
        "print ('Embedding layer output ', embeds_out)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding layer output shape torch.Size([3, 111, 768])\n",
            "Embedding layer output  tensor([[[ 6.8031e-01, -1.1867e+00,  3.2894e-01,  ..., -4.9993e-01,\n",
            "           5.2624e-01,  1.6079e+00],\n",
            "         [ 5.7198e-02,  1.5116e+00,  3.2703e+00,  ...,  1.4151e+00,\n",
            "           1.1917e+00,  1.8828e+00],\n",
            "         [-1.0628e+00,  4.9244e+00,  1.5371e+00,  ..., -2.0137e+00,\n",
            "           2.2094e+00,  8.5177e-01],\n",
            "         ...,\n",
            "         [-1.8299e-01, -3.9856e+00,  1.7900e-01,  ...,  1.3204e+00,\n",
            "          -8.1669e-01,  2.8818e+00],\n",
            "         [-3.6636e+00, -5.3252e+00, -5.1080e+00,  ..., -2.0597e+00,\n",
            "           1.5085e-01,  1.3636e+00],\n",
            "         [ 3.5583e-01,  5.7035e-01,  9.2911e-02,  ...,  1.8058e-03,\n",
            "          -3.7883e-02, -3.7180e-01]],\n",
            "\n",
            "        [[-6.5352e-01, -7.4600e-01,  6.9935e-01,  ..., -8.9726e-01,\n",
            "          -5.0028e-01,  1.3217e+00],\n",
            "         [-8.0479e-01,  2.5052e+00,  5.1342e+00,  ...,  1.7071e-01,\n",
            "           1.3210e+00,  2.1838e-01],\n",
            "         [ 3.1624e+00, -3.5378e+00,  1.6601e+00,  ...,  1.6580e-01,\n",
            "           8.0771e-01,  2.8724e-02],\n",
            "         ...,\n",
            "         [ 4.7011e-01, -3.4960e+00,  2.5674e+00,  ...,  4.2880e-01,\n",
            "           1.5010e+00,  1.7249e+00],\n",
            "         [-5.5027e+00, -4.0114e+00, -1.1157e+00,  ..., -1.8008e-01,\n",
            "          -1.3357e+00,  2.2988e+00],\n",
            "         [ 3.4075e-01,  5.2684e-01, -1.9033e-01,  ..., -6.2455e-02,\n",
            "          -6.4120e-02, -2.4756e-01]],\n",
            "\n",
            "        [[ 5.0477e-01, -1.4398e+00,  1.3945e+00,  ..., -3.1823e-01,\n",
            "           9.3505e-01,  2.1852e-01],\n",
            "         [-2.3301e+00,  6.4745e-01,  4.2949e+00,  ..., -1.8453e-01,\n",
            "           1.9172e+00, -9.9050e-01],\n",
            "         [ 1.2760e+00,  3.5589e+00,  9.0913e-01,  ..., -9.8269e-01,\n",
            "           8.7624e-01,  1.2114e+00],\n",
            "         ...,\n",
            "         [ 3.2122e+00, -4.0502e+00,  2.7111e+00,  ..., -1.9349e+00,\n",
            "           3.0566e+00, -3.4027e+00],\n",
            "         [ 1.0166e+00, -1.6670e+00,  3.1130e+00,  ..., -2.1220e+00,\n",
            "           1.9715e+00, -3.3379e+00],\n",
            "         [ 4.4816e+00,  1.0085e+00,  2.9029e+00,  ..., -2.6723e+00,\n",
            "          -3.8432e-01, -4.2482e+00]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MO37SnMiqByG",
        "colab_type": "code",
        "colab": {},
        "outputId": "4dae80d9-947e-4154-a53e-4c04dfc27e2b"
      },
      "source": [
        "batch_size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 239
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmTBzIn5qByN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuqubS20qByP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1d785d8c-15cb-4a16-cd25-c44ff06eac30"
      },
      "source": [
        "it = 0\n",
        "tot = 0\n",
        "act = 0\n",
        "\n",
        "PRINT_EVERY = 10\n",
        "\n",
        "from tqdm import tqdm\n",
        "loss_function = nn.NLLLoss()\n",
        "for i in range(len(train_label)//batch_size):\n",
        "    idx = [i*batch_size + k for k in range(batch_size)]\n",
        "    batch = train_text[idx]\n",
        "    y = train_label[idx]\n",
        "    y = torch.from_numpy(np.array((y)))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    pred = model(batch)\n",
        "    print (pred.shape)\n",
        "    l = loss_function(pred, y)\n",
        "    l.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    it += 1\n",
        "    tot += l.item()\n",
        "    act += (pred.detach().numpy().argmax() == y)\n",
        "    if it == PRINT_EVERY:\n",
        "        print(tot/it, act/it)\n",
        "        it = 0\n",
        "        tot = 0\n",
        "        act = 0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([20, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D7pjibcqByS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}